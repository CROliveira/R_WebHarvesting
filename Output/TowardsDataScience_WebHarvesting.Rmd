---
title: "Towards Data Science, webscrapping for fun"
subtitle: "How to dynamically read and optmize skimming processes"
author: "Celio Oliveira"
contact: "oliveira.celior@gmail.com"
cate: format(Sys.time(), '%d %B %Y')
pre-requisites: "None"
output:bookdown::pdf_document2:toc: yes
thanks: "Code and data are available at: https://github.com/CROliveira/R_WebHarvesting"
bibliography: references.bib
output:
  # word_document: default
  # pdf_document: default
  html_document: 
    df_print: paged
always_allow_html: true
---

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;


\tableofcontents

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;




```{r setup, include=FALSE}
## Setting up the environment
knitr::opts_chunk$set( echo = TRUE, fig.path = "figures/")

# Check.packages function: install and load multiple R packages.
check_for_packages <- function( pkg){
  # Check to see if packages are installed. Install them if they are not, then load them into the R session.  
  new.pkg <- pkg[!( pkg %in% installed.packages()[, "Package"])]
    if ( length( new.pkg)) 
        install.packages( new.pkg, dependencies = TRUE)
    sapply( pkg, require, character.only = TRUE) 
}

# Passing the library calls to the check.packages function
packages<-c( "tidyverse", "here", "dplyr", "ggplot2",  "ggmap", "kableExtra", "rvest", "webshot", "stringr") # webshot is a helper for html files
# webshot::install_phantomjs(force = TRUE) # Installing a helper library for html files

check_for_packages( packages) # Returns TRUE if packages are installed, FALSE otherwise

here::here() # Setting a work directory

```




# Web harvesting :: Towards Data Science

This document is a starter of a web harvesting project that aims to optimize the search for articles on the website [Towards Data Science]("https://towardsdatascience.com"). Towards DS is a platform using [Medium]("https://medium.com") that exchanges ideas and expands the understanding of data science. It has a mixed audience, consisting of readers entirely new to the subject and expert professionals who want to share their inventions and discoveries.

This document was inspired in [@TowardsDataScience] and [@VaexWebHarvesting]. It was analyzed using R [@citeR], the "tidyverse" package written by [@tidyverse], "dplyr" package written by [@dplyr], and "rvest" written by [@rvest].

\newpage


## Introduction


This sample brings the last publications on the landing page of Towards DS website using a rvast package that involves creating an object that we can use to parse the HTML from a webpage. Furthermore, rvest can connect to a webpage and scrape / parse its HTML in a single package. We use syntax similar to dplyr and other tidyverse packages by using %>%.

In further phases, I aim to filter the title of publications, author name, date of publishing, last updated on, how many claps the article received, etc. It is not as easy as I though working with scraped data in R and I need to spend some more time on the documentation and understanding of the parameters to get the data I intend to.

The table below shows the last publications uploaded on the website:

```{r Fetching the data, include = TRUE, echo = FALSE}
# get HTML object
url <- "https://towardsdatascience.com"
titles <- read_html(url) %>% html_nodes('h1') %>%
  html_nodes('a') %>% html_text()

titles_df <- data.frame(titles)
titles_df
```




```{r TowardsDS webpage, include=FALSE, echo = FALSE, message = FALSE}
# author <- (read_html(url) %>% 
#   html_nodes('div') %>%
#   html_nodes('h4') %>% 
#   html_text())[2]
# 
# update <- (read_html(url) %>% 
#   html_nodes('div') %>%
#   html_nodes('h4') %>% 
#   html_text())[3]

# w <- (read_html(url) %>% 
#   html_nodes('div') %>%
#   html_nodes('h4') %>% 
#   html_text())[8]
# 
# # titles
# urls <- read_html(url) %>%
#   html_nodes('.image_container') %>% 
#   html_nodes('a') %>% 
#   html_attr('href') %>% 
#   str_replace_all('../../../', '/')
# 
# imgs <- read_html(url) %>% 
#   html_nodes('.image_container') %>%
#   html_nodes('img') %>%
#   html_attr('src') %>%
#   str_replace_all('../../../../', '/')
# 
# ratings <- read_html(url) %>% 
#   html_nodes('p.star-rating') %>% 
#   html_attr('class') %>% 
#   str_replace_all('star-rating ', '')
# 
# prices <- read_html(url) %>% 
#   html_nodes('.product_price') %>% 
#   html_nodes('.price_color') %>% 
#   html_text()
# 
# availability <- read_html(url) %>% 
#   html_nodes('.product_price') %>% 
#   html_nodes('.instock') %>% 
#   html_text() %>% 
#   str_trim()
# 
# scraped <- data.frame(
#   Title = titles, 
#   Author = author, 
#   Updated = update
# )
# scraped
```

&nbsp;
&nbsp;
&nbsp;


## References


